{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Pickling\n",
    "\n",
    "---\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "- Learn what serialization and deserialization are\n",
    "- Learn what \"pickling\" is in Python\n",
    "- Review using `with` statements to safely handle file operations\n",
    "- Pickle and unpickle scikit-learn models in Python\n",
    "\n",
    "---\n",
    "\n",
    "## What is pickling?\n",
    "\n",
    "If you're talking about food, pickling is a method of preserving food for the future. If you're talking about Python, pickling is a method of preserving **objects** for the future, including functions and classes. Since sklearn models are instances of classes, they can be pickled.\n",
    "\n",
    "To pickle an object, it needs to be **serialized**. Serialization is when we transform an object into byte streams. (Byte streams are collections of bytes. One byte is made up of eight zeros or ones.) To unpickle an object so that it can be used in Python again, it needs to be **deserialized**.\n",
    "\n",
    "If you've ever saved your progress in a video game, you've already serialized data without knowing it. A save file is your serialized save state. When you load the save, you deserialize the data so you can resume the game right where you were before you quit.\n",
    "\n",
    "### Some warnings:\n",
    "\n",
    "Just like you can't open a [Pokemon: Red](https://en.wikipedia.org/wiki/Pok%C3%A9mon_Red_and_Blue) savefile in [Pokemon: Sun](https://en.wikipedia.org/wiki/Pok%C3%A9mon_Sun_and_Moon), you have to unpickle an object in the same version of Python that you pickled it in. \n",
    "\n",
    "**Pickle objects can contain malicious code**. Never unpickle an object you don't trust!\n",
    "\n",
    "## Why pickle?\n",
    "\n",
    "Pickling makes a lot of sense any time you have a model you want to work with that you don't want to refit. Today, we'll pickle a fitted pipeline so that we can import it into a Streamlit web app, but pickling is useful in many other situations as well.\n",
    "\n",
    "If you have a model that took twelve hours to fit, you might want to analyze its residuals, work with its coefficients, or make predictions off of it. But without saving it some fashion, you'd need to refit the model every time you restarted your notebook. Pickling the model allows you to load the fitted model _without_ needing to re-run the code where you fit it.\n",
    "\n",
    "Notes:\n",
    "- Pickling does **not** compress your model, meaning that some pickled models can end up being fairly large file sizes. Think of K-nearest neighbors, which requires every data point to be stored inside the model (though sklearn _does_ optimize the way the data is stored for speed and efficiency, models can still be large.) \n",
    "- Keras has its own [`save` method](https://www.tensorflow.org/guide/keras/save_and_serialize) on models. If you want to save a neural network fit in keras, use that instead of pickle.\n",
    "- Don't pickle data frames. Export them as csv files instead. Generally, if there's another way to save something, use it.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickling a simple datatype\n",
    "\n",
    "Before we pickle a full model, let's demonstrate pickling on a simple list.\n",
    "\n",
    "Create a list called `my_vegetables` that contains some strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vegetables = ['cucumber', 'red pepper', 'onion', 'beet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the pickled list to disk\n",
    "\n",
    "Let's review [this link](https://www.pythonforbeginners.com/files/with-statement-in-python) to go over why `with` is such a good tool for file operations.\n",
    "\n",
    "Let's use `with` to write the list to disk as a `.pkl` file. We'll need to use `open`, pass in a file name, and also tell Python we're **writing** to the file, and writing as **bytes**. The pickle method we'll use is called `dump`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('veggies.pkl', 'wb') as f:\n",
    "    pickle.dump(my_vegetables, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open the pickled list\n",
    "\n",
    "Let's use `with` to open the pickled file and save it as a new variable, `list_from_pickle`. Remember to tell Python that we're **reading** from the file, and that we're reading in **bytes**. The pickle method we'll use is called `load`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('veggies.pkl', 'rb') as f:\n",
    "    list_from_pickle = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cucumber', 'red pepper', 'onion', 'beet']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pickle a fitted pipeline\n",
    "\n",
    "Let's start by building a model to determine whether someone writes more like [Edgar Allen Poe](https://en.wikipedia.org/wiki/Edgar_Allan_Poe) or [Jane Austen](https://en.wikipedia.org/wiki/Jane_Austen).\n",
    "\n",
    "Our end goal will be a fitted pipeline. But before we export our pipeline, we'll need to settle on a model.\n",
    "\n",
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SENSE AND SENSIBILITY</td>\n",
       "      <td>Jane Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>by Jane Austen</td>\n",
       "      <td>Jane Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(1811)</td>\n",
       "      <td>Jane Austen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     text       author\n",
       "0   SENSE AND SENSIBILITY  Jane Austen\n",
       "1          by Jane Austen  Jane Austen\n",
       "2                  (1811)  Jane Austen"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/austen_poe.csv').dropna()\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Jane Austen        0.607353\n",
       "Edgar Allan Poe    0.392647\n",
       "Name: author, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['author'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    stratify=y, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('tf', TfidfVectorizer(min_df=2)),\n",
    "    ('lr', LogisticRegressionCV(solver='liblinear'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tf', TfidfVectorizer(min_df=2)),\n",
       "                ('lr', LogisticRegressionCV(solver='liblinear'))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9932717190388171, 0.9571967176757596)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.score(X_train, y_train), pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model decision: count vectorizer vs TF-IDF vectorizer\n",
    "\n",
    "Recall that a count vectorizer converts documents into vector representations of word occurrences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(min_df=2)\n",
    "cv.fit(X_train)\n",
    "\n",
    "cv_text = cv.transform(X_train)\n",
    "# remember to use .todense() to de-sparsify the count vectorized text\n",
    "cv_text_df = pd.DataFrame(cv_text.todense(), columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13525, 17330)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to perform EDA on word counts, it may be useful to add the original author's name as a column:\n",
    "\n",
    "**Note**: It might be a mistake to add this information as `cv_text_df['author']` or `cv_text_df['label']`. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>10th</th>\n",
       "      <th>11</th>\n",
       "      <th>11th</th>\n",
       "      <th>12</th>\n",
       "      <th>12mo</th>\n",
       "      <th>13</th>\n",
       "      <th>13th</th>\n",
       "      <th>14</th>\n",
       "      <th>...</th>\n",
       "      <th>zigzag</th>\n",
       "      <th>zit</th>\n",
       "      <th>zoar</th>\n",
       "      <th>zoilus</th>\n",
       "      <th>zone</th>\n",
       "      <th>zäire</th>\n",
       "      <th>ælfric</th>\n",
       "      <th>æronaut</th>\n",
       "      <th>être</th>\n",
       "      <th>author_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Edgar Allan Poe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Jane Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Edgar Allan Poe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 17331 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000  10  10th  11  11th  12  12mo  13  13th  14  ...  zigzag  zit  zoar  \\\n",
       "0    0   0     0   0     0   0     0   0     0   0  ...       0    0     0   \n",
       "1    0   0     0   0     0   0     0   0     0   0  ...       0    0     0   \n",
       "2    0   0     0   0     0   0     0   0     0   0  ...       0    0     0   \n",
       "\n",
       "   zoilus  zone  zäire  ælfric  æronaut  être     author_label  \n",
       "0       0     0      0       0        0     0  Edgar Allan Poe  \n",
       "1       0     0      0       0        0     0      Jane Austen  \n",
       "2       0     0      0       0        0     0  Edgar Allan Poe  \n",
       "\n",
       "[3 rows x 17331 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_text_df['author_label'] = y_train.values\n",
    "cv_text_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "> **Reminder**: \"Document\" means \"one natural language observation.\" Here, it means \"one paragraph from either Jane Austen or Edgar Allen Poe.\" \"Corpus\" means \"the whole natural language dataset that we're using\" -- so here it means \"the collected works of Jane Austen and Edgar Allen Poe, as scraped from Gutenberg and split into paragraphs.\"\n",
    "\n",
    "An alternative to count vectorization is **TF-IDF vectorization**, where TF-IDF stands for \"term frequency-inverse document frequency.\" Instead of just counting the words in each document, TF-IDF _weights_ the words in each document.\n",
    "\n",
    "The general idea behind TF-IDF is that words used many times in a document should matter more, unless they're also used many times in very many documents across the corpus! The TF-IDF thus weights words by both the **term frequency**, which is the number of times the word is used in the document, and the **inverse document frequency**, which measures how important a term is across the corpus. To compute the TF-IDF of one word used in one document, we divide the term frequency by the inverse document frequency. We do this for each word in each document.\n",
    "\n",
    "The formula for the term frequency of a term is written as\n",
    "\n",
    "$$\n",
    "\\text{tf}(t, d) = t/n\n",
    "$$\n",
    "\n",
    "where $t$ is the number of times the term is used in the document, and $n$ is the total number of words in the document.\n",
    "\n",
    "The formula for inverse document frequency is written as\n",
    "\n",
    "$$\n",
    "\\text{idf}(t) = \\log{\\frac{n}{1+\\text{df}(t)}}\n",
    "$$\n",
    "\n",
    "where $n$ is the number of documents in the corpus, and $\\text{df}(t)$ is the number of documents in the corpus that contain the term $t$.\n",
    "\n",
    "The TF-IDF itself is then computed as\n",
    "\n",
    "$$\n",
    "\\text{tf}(t,d) \\times \\text{idf}(t)\n",
    "$$\n",
    "\n",
    "which is the product of the term frequency and the inverse document frequency.\n",
    "\n",
    "> **Note**: There are a few other ways to formulate a TF-IDF. Some other implementations may calculate the inverse document frequency differently. You can see other formulations [here](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Definition). The formula above is the most common implementation and is the implementation used by scikit-learn.\n",
    "\n",
    "### TF-IDF in scikit-learn\n",
    "\n",
    "The `TfidfVectorizer` functions very similarly to `CountVectorizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=2)\n",
    "tfidf.fit(X_train)\n",
    "\n",
    "tfidf_text = tfidf.transform(X_train)\n",
    "# remember to use .todense() to de-sparsify the count vectorized text\n",
    "tfidf_text_df = pd.DataFrame(tfidf_text.todense(), columns=tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it does not make sense to sum the terms of the TF-IDF representation of our data.\n",
    "\n",
    "We can still do some exploration of the TF-IDF scores! The TF-IDF vectorizer has a fitted attribute `.idf_` which stores the inverse document frequency for each word in the corpus. Here, we will construct a data frame of the words in the corpus, and pair them with their IDF scores. Which words have large IDF scores, and which words have small IDF scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17330"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = tfidf.get_feature_names_out()\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17330,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17330"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_df = pd.DataFrame(zip(vocab, tfidf.idf_),\n",
    "                    columns=[\"Vocabulary\", \"IDF\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vocabulary</th>\n",
       "      <th>IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8665</th>\n",
       "      <td>irrecoverably</td>\n",
       "      <td>9.413757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3982</th>\n",
       "      <td>curt</td>\n",
       "      <td>9.413757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>cushioned</td>\n",
       "      <td>9.413757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3993</th>\n",
       "      <td>curvetted</td>\n",
       "      <td>9.413757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3988</th>\n",
       "      <td>curtseyed</td>\n",
       "      <td>9.413757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3986</th>\n",
       "      <td>curtis</td>\n",
       "      <td>9.413757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11633</th>\n",
       "      <td>pleiads</td>\n",
       "      <td>9.413757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11635</th>\n",
       "      <td>plentifully</td>\n",
       "      <td>9.413757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11637</th>\n",
       "      <td>pliancy</td>\n",
       "      <td>9.413757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3980</th>\n",
       "      <td>cursing</td>\n",
       "      <td>9.413757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Vocabulary       IDF\n",
       "8665   irrecoverably  9.413757\n",
       "3982            curt  9.413757\n",
       "3995       cushioned  9.413757\n",
       "3993       curvetted  9.413757\n",
       "3988       curtseyed  9.413757\n",
       "3986          curtis  9.413757\n",
       "11633        pleiads  9.413757\n",
       "11635    plentifully  9.413757\n",
       "11637        pliancy  9.413757\n",
       "3980         cursing  9.413757"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_df.sort_values(by=\"IDF\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vocabulary</th>\n",
       "      <th>IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1808</th>\n",
       "      <td>be</td>\n",
       "      <td>2.044576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16821</th>\n",
       "      <td>was</td>\n",
       "      <td>1.992579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10536</th>\n",
       "      <td>not</td>\n",
       "      <td>1.943533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15381</th>\n",
       "      <td>that</td>\n",
       "      <td>1.871720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8712</th>\n",
       "      <td>it</td>\n",
       "      <td>1.833738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8096</th>\n",
       "      <td>in</td>\n",
       "      <td>1.697741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10706</th>\n",
       "      <td>of</td>\n",
       "      <td>1.485952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15604</th>\n",
       "      <td>to</td>\n",
       "      <td>1.476858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>and</td>\n",
       "      <td>1.472580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15382</th>\n",
       "      <td>the</td>\n",
       "      <td>1.408835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Vocabulary       IDF\n",
       "1808          be  2.044576\n",
       "16821        was  1.992579\n",
       "10536        not  1.943533\n",
       "15381       that  1.871720\n",
       "8712          it  1.833738\n",
       "8096          in  1.697741\n",
       "10706         of  1.485952\n",
       "15604         to  1.476858\n",
       "1092         and  1.472580\n",
       "15382        the  1.408835"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_df.sort_values(by=\"IDF\", ascending=False).tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comparing models\n",
    "\n",
    "We could use either the TF-IDF vectorizer or the count vectorizer, and one might work better than the other, so let's try both - and let's try both alongside logistic regression and multinomial Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9911275415896488, 0.9467731204258151)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipel = Pipeline([\n",
    "    (\"cv\", CountVectorizer()),\n",
    "    (\"lr\", LogisticRegressionCV(solver=\"liblinear\"))\n",
    "])\n",
    "\n",
    "pipel.fit(X_train, y_train)\n",
    "pipel.score(X_train, y_train), pipel.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.994011090573013, 0.9580838323353293)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe2 = Pipeline([\n",
    "    (\"tf\", TfidfVectorizer()),\n",
    "    (\"lr\", LogisticRegressionCV(solver=\"liblinear\"))\n",
    "])\n",
    "\n",
    "pipe2.fit(X_train, y_train)\n",
    "pipe2.score(X_train, y_train), pipe2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9510536044362292, 0.9387890884896873)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe3 = Pipeline([\n",
    "    (\"cv\", CountVectorizer()),\n",
    "    (\"nb\", MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe3.fit(X_train, y_train)\n",
    "pipe3.score(X_train, y_train), pipe3.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9191127541589649, 0.8942115768463074)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe4 = Pipeline([\n",
    "    (\"tf\", TfidfVectorizer()),\n",
    "    (\"nb\", MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe4.fit(X_train, y_train)\n",
    "pipe4.score(X_train, y_train), pipe4.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "My best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.994011090573013, 0.9580838323353293)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"tf\", TfidfVectorizer()),\n",
    "    (\"lr\", LogisticRegressionCV(solver=\"liblinear\"))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_train, y_train), pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Pickle a fitted pipeline\n",
    "\n",
    "\n",
    "### Export the fitted pipeline to `models` as `author_pipe.pkl`\n",
    "\n",
    "This time, let's export to the `models` folder in this repository.\n",
    "\n",
    "> Why bother? First, if you'll have lots of serialized objects, or if your serialized objects take up lots of disk space, you might not want to add and commit them to Github. Keeping them all in the same folder makes it easier to stay organized and not commit them. Second, it's also just a good way to organize a repository!\n",
    "\n",
    "Just like before, we'll use a `with` statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wb: Write Binary\n",
    "# When pickle.dump() \n",
    "with open('../models/author_pipe.pkl', 'wb') as f:\n",
    "    pickle.dump(pipe, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll try to un-pickle in the `02-read_a_pickle` notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
